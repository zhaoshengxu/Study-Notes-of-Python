# 老王Python学习笔记
> 运行环境：
>- VirtualBox4.8.0 /Ubuntu16.04.1 /x86_64(Python2.7)
>- 代码编辑器sublime text 3
## 二、进阶篇

### 进阶篇27-28-高性能的多线程网络资源访问
多线程爬虫


1. 抓什么？怎么抓？ 
爬虫的第一步是分析-> 确定抓取流程
> 先实现功能，再优化细节。
2. 分析的工具 firebug 确定最小单元
>- 谷歌浏览器firebug插件安装：
[Chrome中的Firebug插件：Firebug Lite](http://chromecj.com/web-development/2015-05/471.html)
>- 谷歌浏览器自带的<开发者工具>打开方式：F12或者ctrl+shift+I
3. 爬取html比较常用的三个包：urllib/requests/beautifulsoup
> 以下分别为爬取html的三种方式，爬去的内容可能有所不同，因此需要根据实际情况选取合适的方法。
```
# urllib
import urllib.request
content = urllib.request.urlopen('http://money.163.com/special/pinglun/')
# char = content.info().get_content_charset() # 获取网页编码方式1
char = content.headers.get_content_charset()  # 获取网页编码方式2
c = content.read().decode(char)# 把网页内容从字节型解码为字符串类型
print(c)

# requests
import requests
req = requests.get('http://money.163.com/special/pinglun/')
html = req.text
print(html)

# beautifulsoup
import urllib.request
from bs4 import BeautifulSoup
content = urllib.request.urlopen('http://money.163.com/special/pinglun/').read()  
soup = BeautifulSoup(content, 'lxml')
print(soup)
```

4. 分析数据，正则 or beautifulsoup
> [[python] 常用正则表达式爬取网页信息及分析HTML标签总结](http://blog.csdn.net/eastmount/article/details/51082253)

5. beautifulsoup的小入门
> [bs4中文文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id4)

6. scrapy小入门
>- [Scrapy 1.4 documentation](https://doc.scrapy.org/en/latest/intro/tutorial.html)
>- [python爬虫xpath的语法](http://www.cnblogs.com/lonenysky/p/4649455.html)
>- [scrapy学习笔记](http://python.jobbole.com/86584/)
>- [ scrapy爬取京东商城某一类商品的信息和评论（一）](http://blog.csdn.net/xiaoquantouer/article/details/51840332)

7. 爬取含有中文网页，返回结果显示中文：把下边这行代码添加到scrapy根目录下的settings.py文件中保存即可。
```
FEED_EXPORT_ENCODING = 'utf-8'
```

8. 从ubuntu终端进入Python和scrapy shell，再退回到终端的快捷键都是：Ctrl + D

9. 像字典的字符串，转化为字典的方法：方法一：eval()；方法二：json.loads()
```
import json
s = '{"id":"J_280930","p":"279.00","m":"300.00","op":"279.00"}'
a = eval(s)  # 方法1
b = json.loads(s)  # 方法2
print(type(s) == str)
print(type(a) == type(a) == dict)
print(a == b)
print(a)
>>> True
True
True
{'p': '279.00', 'm': '300.00', 'id': 'J_280930', 'op': '279.00'}
```
10. 使用以下命令在shell终端中以窗口形式打开一个文件夹：
```
nautilus  dirname
```
> 需要首先安装nautilus包，按照提示命令安装即可。安装命令为：

```
sudo apt install nautilus
```

11. 题外话: [Mashup百科](http://baike.baidu.com/link?url=3Y3pIl5x_EC1YXKytmH6mRP0HI6eeTXyPqKYePSdOl0fqkBMQNFbh_R9kDCIrEzqe5g2GkYbIWQHRg4il6RHza)


### 习题

1. 作业1：
```
'''
作业：使用beautifulsoup抓取相应内容
url :"http://money.163.com/special/pinglun/"
抓取第一页的新闻信息，并按照以下规格输出。
{'title':'','created_at':'','url':''}
@难点：网页内容分析，锁定目标单元，确定单元目标内容的取出方法
'''
# 方法1
import urllib.request
from bs4 import BeautifulSoup
content = urllib.request.urlopen('http://money.163.com/special/pinglun/').read()  # 获取网页内容
soup = BeautifulSoup(content, 'lxml')     # 解析整个网页内容
item_top = soup.find_all('div', class_="list_item clearfix")   # 锁定目标网页内容
tcu = []
for i in item_top:  # for循环遍历每一个<div class_="list_item clearfix>
    # 利用tag对象的属性取出网页地址、新闻标题和创建时间
    tcu.append({'url': i.a['href'], 'title': i.a.string, 'created_at': i.span.string})
print(tcu)


# 方法2：与方法1相比，仅解析部分网页内容，并进一步缩小查找范围
import urllib.request
from bs4 import BeautifulSoup, SoupStrainer
content = urllib.request.urlopen('http://money.163.com/special/pinglun/').read()  # 获取网页内容
list_item = SoupStrainer('div', class_="list_item clearfix")  # 利用firebug确定目标网页内容
soup = BeautifulSoup(content, 'lxml', parse_only=list_item)  # 仅解析目标网页内容，而非整个网页
item_top = soup.find_all('div', class_="item_top")  # 进一步缩小查找范围
tcu = []
for i in item_top:
    tcu.append({'url': i.a['href'], 'title': i.a.string, 'created_at': i.span.string})
print(tcu)
```
---

2. 作业2：
```
'''
@作业：使用bs4从京东爬取商品数据
url: "http://search.jd.com/Search?keyword=%E5%B9%BC%E7%8C%AB%E7%8C%AB%E7%B2%AE&enc=utf-8#filter"
print jd_search(keyword)
[dict,dict,dict]
dict {pic:'',title:'',price:'',url:''}
'''
import urllib.request
from bs4 import BeautifulSoup


def jd_search(url):
    '''
    @功能：从京东爬取商品名称、图片、价格、网址信息
    @问题：1. 图片网址抓取不全；2. 单个网页共有60条商品信息，仅爬取30条
    '''
    content = urllib.request.urlopen(url).read()  # 读取网页内容
    soup = BeautifulSoup(content, 'lxml')  # 解析网页内容
    gl_i_wrap = soup.find_all('div', class_="gl-i-wrap")  # 缩小搜索范围

    ptpu = []
    for x in gl_i_wrap:  # 遍历每一个目标单元
        # 网页部分内容未解析出来，判断标签属性是否存在，不存在返回None
        title = x.div.a['title']
        url = x.div.a['href']

        if x.div.a.img.has_attr('src'):
            pic = x.div.a.img['src']
        else:
            pic = None

        if x.find('div', class_='p-price').strong.has_attr('data-price'):
            price = x.find('div', class_='p-price').strong['data-price']
        else:
            price = None

        d = {'pic': pic, 'title': title, 'price': price, 'url': url}
        ptpu.append(d)
    return ptpu


url = "http://search.jd.com/Search?keyword=%E5%B9%BC%E7%8C%AB%E7%8C%AB%E7%B2%AE&enc=utf-8#filter"
print(jd_search(url))
```
---

3. 作业3：
```
'''
使用scrapy完成作业2的需求。
jd_search(keyword,page_skip=1,page_limit=10) #抓1后面10页（包括第10页）的内容。
jd_search(keyword,page_skip=4,page_limit=3) #抓4后面3页（包括第6页）的内容。
'''
# coding=utf-8
import scrapy
from tutorial.items import goodsItem  # 预先创建的Item对象（和字典类似）存储结构化数据


class jd_n(scrapy.Spider):
    name = 'jd_search'

    start_urls = []
    for i in range(1, 11):   # 设置页数，目前只能抓取猫粮分类下前10页的商品
        url = 'http://list.jd.com/list.html?cat=6994,6995,7003&page=%d' % i
        start_urls.append(url)

    def parse(self, response):
        for g in response.css("li.gl-item"):
            gooditem = goodsItem()
            gooditem['ID'] = g.xpath("./div/@data-sku").extract_first()
            gooditem['picture'] = g.xpath("./div/div[@class='p-img']/a/img/@src").extract_first()
            gooditem['name'] = g.xpath("./div/div[@class='p-name']/a/em/text()").extract_first()
            gooditem['link'] = g.xpath("./div/div[@class='p-name']/a/@href").extract_first()
            url = "https://p.3.cn/prices/mgets?callback=jQuery&skuIds=J_%s" % gooditem['ID']
            yield scrapy.Request(url, meta={'item': gooditem}, callback=self.parse_price)

    def parse_price(self, response):
        gooditem = response.meta['item']
        tmp = response.body[8:-4]  # 获取到需要的内容, str
        d = eval(tmp)              # str --> dict
        gooditem['price'] = d['p']
        return gooditem


'''
@Question:
1. 单个网页共有60条商品信息，仅爬取30条
2. 部分商品图片信息爬取不到的问题

@Answer:
1. 更改起始网址，从商品分类进入猫粮主页：宠物生活>猫粮。
猫粮主页："http://list.jd.com/list.html?cat=6994,6995,7003"
2. 一开始源代码里面只能看到30条商品的信息，随着你的下拉，另外30条才会加载出来。图片网址抓取不全可能也是类似的原因。
3. 大多这样的原因是因为运行时动态加载的内容，在你进行解析时还没加载完毕，所以提取不到。
4. 另外，有不少时候，可能使用的不是A链接，而是DIV或SPAN等类似对象的ONCLICK事件。这样的，你只用A链接提取，自然也会被漏掉。
'''
```
---

```
'''
运行环境：win7，配置anaconda，python3.5
'''
# coding=utf-8
import scrapy
import json
from tutorial.items import goodsItem  # 预先创建的Item对象（和字典类似）存储结构化数据


class jd_n(scrapy.Spider):
    name = 'jd_search'

    start_urls = []
    for i in range(1, 3):   # 设置页数，目前只能抓取猫粮分类下前10页的商品
        url = 'http://list.jd.com/list.html?cat=6994,6995,7003&page=%d' % i
        start_urls.append(url)

    def parse(self, response):
        for g in response.css("li.gl-item"):
            gooditem = goodsItem()
            gooditem['ID'] = g.xpath("./div/@data-sku").extract_first()
            gooditem['picture'] = g.xpath("./div/div[@class='p-img']/a/img/@src").extract_first()
            gooditem['name'] = g.xpath("./div/div[@class='p-name']/a/em/text()").extract_first()
            gooditem['link'] = g.xpath("./div/div[@class='p-name']/a/@href").extract_first()
            url = "http://pm.3.cn/prices/pcpmgets?callback=jQuery&skuids=%s&origin=2" % gooditem['ID']
            yield scrapy.Request(url, meta={'item': gooditem}, callback=self.parse_price)

    def parse_price(self, response):
        gooditem = response.meta['item']
        tmp = response.body[8:-4]  # 获取到需要的内容, str
        d = eval(tmp)   # str --> dict
        gooditem['price'] = d['p']
        return gooditem


'''
@Question:
1. 部分商品图片信息爬取不到的问题

@Answer:
1. 更改起始网址，从商品分类进入猫粮主页：宠物生活>猫粮。
猫粮主页："http://list.jd.com/list.html?cat=6994,6995,7003"
2. 一开始源代码里面只能看到30条商品的信息，随着你的下拉，另外30条才会加载出来。图片网址抓取不全可能也是类似的原因。
3. 大多这样的原因是因为运行时动态加载的内容，在你进行解析时还没加载完毕，所以提取不到。
4. 另外，有不少时候，可能使用的不是A链接，而是DIV或SPAN等类似对象的ONCLICK事件。这样的，你只用A链接提取，自然也会被漏掉。
'''
```
