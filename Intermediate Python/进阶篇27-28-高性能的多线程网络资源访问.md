# 老王Python学习笔记
> 运行环境：代码编辑器sublime text 3（配置anaconda，python3.5）
## 二、进阶篇

### 进阶篇27-28-高性能的多线程网络资源访问
多线程爬虫


1. 抓什么？怎么抓？ 
爬虫的第一步是分析-> 确定抓取流程

2. 分析的工具 firebug 确定最小单元
>- 谷歌浏览器firebug插件安装：
[Chrome中的Firebug插件：Firebug Lite](http://chromecj.com/web-development/2015-05/471.html)
>- 谷歌浏览器自带的<开发者工具>打开方式：F12或者ctrl+shift+I
3. 爬取html比较常用的三个包：urllib/requests/beautifulsoup
> 以下分别为爬取html的三种方式，爬去的内容可能有所不同，因此需要根据实际情况选取合适的方法。
```
# urllib
import urllib.request
content = urllib.request.urlopen('http://money.163.com/special/pinglun/')
# char = content.info().get_content_charset() # 获取网页编码方式1
char = content.headers.get_content_charset()  # 获取网页编码方式2
c = content.read().decode(char)# 把网页内容从字节型解码为字符串类型
print(c)

# requests
import requests
req = requests.get('http://money.163.com/special/pinglun/')
html = req.text
print(html)

# beautifulsoup
import urllib.request
from bs4 import BeautifulSoup
content = urllib.request.urlopen('http://money.163.com/special/pinglun/').read()  
soup = BeautifulSoup(content, 'lxml')
print(soup)
```

4. 分析数据，正则 or beautifulsoup
> [[python] 常用正则表达式爬取网页信息及分析HTML标签总结](http://blog.csdn.net/eastmount/article/details/51082253)

5. beautifulsoup的小入门
> [bs4中文文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id4)

6. scrapy小入门
>- [Scrapy 1.4 documentation](https://doc.scrapy.org/en/latest/intro/tutorial.html)
>- [python爬虫xpath的语法](http://www.cnblogs.com/lonenysky/p/4649455.html)
>- [scrapy学习笔记](http://python.jobbole.com/86584/)
>- [ scrapy爬取京东商城某一类商品的信息和评论（一）](http://blog.csdn.net/xiaoquantouer/article/details/51840332)

9. 题外话: [Mashup百科](http://baike.baidu.com/link?url=3Y3pIl5x_EC1YXKytmH6mRP0HI6eeTXyPqKYePSdOl0fqkBMQNFbh_R9kDCIrEzqe5g2GkYbIWQHRg4il6RHza)


### 习题

1. 作业1：
```
'''
作业：使用beautifulsoup抓取相应内容
url :"http://money.163.com/special/pinglun/"
抓取第一页的新闻信息，并按照以下规格输出。
{'title':'','created_at':'','url':''}
@难点：网页内容分析，锁定目标单元，确定单元目标内容的取出方法
'''
# 方法1
import urllib.request
from bs4 import BeautifulSoup
content = urllib.request.urlopen('http://money.163.com/special/pinglun/').read()  # 获取网页内容
soup = BeautifulSoup(content, 'lxml')     # 解析整个网页内容
item_top = soup.find_all('div', class_="list_item clearfix")   # 锁定目标网页内容
tcu = []
for i in item_top:  # for循环遍历每一个<div class_="list_item clearfix>
    # 利用tag对象的属性取出网页地址、新闻标题和创建时间
    tcu.append({'url': i.a['href'], 'title': i.a.string, 'created_at': i.span.string})
print(tcu)


# 方法2：与方法1相比，仅解析部分网页内容，并进一步缩小查找范围
import urllib.request
from bs4 import BeautifulSoup, SoupStrainer
content = urllib.request.urlopen('http://money.163.com/special/pinglun/').read()  # 获取网页内容
list_item = SoupStrainer('div', class_="list_item clearfix")  # 利用firebug确定目标网页内容
soup = BeautifulSoup(content, 'lxml', parse_only=list_item)  # 仅解析目标网页内容，而非整个网页
item_top = soup.find_all('div', class_="item_top")  # 进一步缩小查找范围
tcu = []
for i in item_top:
    tcu.append({'url': i.a['href'], 'title': i.a.string, 'created_at': i.span.string})
print(tcu)
```
---

2. 作业2：
```
'''
@作业：使用bs4从京东爬取商品数据
url: "http://search.jd.com/Search?keyword=%E5%B9%BC%E7%8C%AB%E7%8C%AB%E7%B2%AE&enc=utf-8#filter"
print jd_search(keyword)
[dict,dict,dict]
dict {pic:'',title:'',price:'',url:''}
'''
import urllib.request
from bs4 import BeautifulSoup


def jd_search(url):
    '''
    @功能：从京东爬取商品名称、图片、价格、网址信息
    @问题：网页部分内容未解析出来，需要做判断
    '''s
    content = urllib.request.urlopen(url).read()  # 读取网页内容
    soup = BeautifulSoup(content, 'lxml')  # 解析网页内容
    gl_i_wrap = soup.find_all('div', class_="gl-i-wrap")  # 缩小搜索范围

    ptpu = []
    for x in gl_i_wrap:  # 遍历每一个目标单元
        # 网页部分内容未解析出来，判断标签属性是否存在，不存在返回None
        if x.div.a.img.has_attr('src'):
            pic = x.div.a.img['src']
        else:
            pic = None
        if x.div.a.has_attr('title'):
            title = x.div.a['title']
        else:
            title = None
        if x.div.a.has_attr('href'):
            url = x.div.a['href']
        else:
            url = None
        if x.find('div', class_='p-price').strong.has_attr('data-price'):
            price = x.find('div', class_='p-price').strong['data-price']
        else:
            price = None
        d = {'pic': pic, 'title': title, 'price': price, 'url': url}
        ptpu.append(d)
    return p


url = "http://search.jd.com/Search?keyword=%E5%B9%BC%E7%8C%AB%E7%8C%AB%E7%B2%AE&enc=utf-8#filter"
print(jd_search(url))
```
---


---
3. 作业3：
```
'''
使用scrapy完成作业2的需求。
jd_search(keyword,page_skip=1,page_limit=10) #抓1后面10页（包括第10页）的内容。
jd_search(keyword,page_skip=4,page_limit=3) #抓4后面3页（包括第6页）的内容。
'''




```